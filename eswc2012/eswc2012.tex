\documentclass[runningheads,a4paper]{llncs}
% proper encoding
\usepackage[T1]{fontenc}

% autoref command
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}

% better typography
\usepackage[activate=compatibility]{microtype}

% graphics
\usepackage{graphicx}

% URLs
\usepackage{url}

\begin{document}

\mainmatter

\title{Enriching Content Objects for Multimodal Search with Data from the Linking Open Data Cloud}
\authorrunning{Enriching Content Objects for Multimodal Search with Data from the Linking Open Data Cloud}

\author{Jonas Etzold\inst{1} \and Thomas Steiner\inst{2} \and Arnaud Brousseau\inst{2}\thanks{The author was an intern at Google Germany GmbH at time of core development of the application.} \and Paul Grimm\inst{1}}

\institute{\label{fulda}Hochschule Fulda, Marquardstr. 35
36039 Fulda, Germany\\
\urldef{\emails}\path|{jonas.etzold,paul.grimm}@hs-fulda.de|\emails\\
\and Google Germany GmbH, ABC-Str. 19, 20355 Hamburg, Germany\\
\urldef{\emails}\path|{tomac,arnaudb}@google.com|\emails\\
}

\maketitle

\begin{abstract}
In this paper, we report on work around the \mbox{I-SEARCH} EU (FP7 ICT STREP) project
whose objective is the development of a multimodal search engine that supports multimodal
in- and output, as well as multimodal  query refinement.
Supported modalities and combinations thereof are \emph{audio}, \emph{video},
\emph{rhythm}, \emph{image}, \emph{3D object}, \emph{sketch}, \emph{emotion},
\emph{social signals}, \emph{geolocation}, and \emph{text}.
An important aspect for \mbox{I-SEARCH} to work is the so-called
\emph{Rich Unified Content Description} \emph{\mbox{(RUCoD)}} format
consisting of a multi-layered structure
for the description of low and high level features of content objects.
The \emph{\mbox{(RUCoD)}} description format allows content objects
to be queried in a consistent way by using extracted \mbox{\emph{RUCoD}} features.
During the session, we will present a live demonstration of the \mbox{I-SEARCH}
Graphical User Interface (GUI) prototype and, via pre-defined use cases,
show how we imagine multimodal search in the future.
We are especially looking for networking opportunities with projects dealing with
semantic annotation of large-scale multimedia content pools and 
projects interested in our \emph{RUCoD} feature extraction techniques.
\end{abstract}

\bibliographystyle{splncs03}
\bibliography{eswc2012}

\end{document}