\documentclass[runningheads,a4paper]{llncs}
\usepackage[utf8]{inputenc}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{eurosym}
\usepackage[normalem]{ulem}
\usepackage{alltt}
\usepackage{subfig}
\usepackage{amssymb}
\setcounter{tocdepth}{3}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Context-aware Querying for\\ Multi-modal Search Engines}

% a short form should be given in case it is too long for the running head
\titlerunning{Context-aware Querying for Multi-modal Search Engines}

\author{Jonas Etzold\inst{1} \and Arnaud Brousseau\inst{2} \and Paul Grimm\inst{1} \and Thomas Steiner\inst{2}}

\authorrunning{Context-aware Querying for Multi-modal Search Engines}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Erfurt University of Applied Sciences, Germany, \email{\{jonas.etzold|grimm\}@fh-erfurt.de}
\and Google Germany GmbH, ABC-Str. 19, 20354 Hamburg, Germany, \email{\{arnaudb|tomac\}@google.com}}

\maketitle

\begin{abstract}
(Tom)
\keywords{lorem, ipsum}
\end{abstract}

\section{Introduction}
(Tom) \cite{ijmis}

\section{Related Work}
(Jonas) \cite{nigay}

\section{Methodology}
In this Section we present our methodology for context-aware querying for multi-modal search engines, split up in three sub-tasks \emph{MMBag}, \emph{UIIFace}, and \emph{CoFind}.

\subsection{MMBag}
MMBag stands for Multi-Media Bag and designates the I~SEARCH User Interface. It comes with specific requirements linked with the need for users to use multiple types of input: audio files or stream, video files, 3D objects, hand drawings, real-world information such as geolocation or time, image files and of course, plain text. This part of the paper shows the approach chosen to create MMBag.

Multi-modal search engines are still very experimental at the time of writing. When building MMBag, we tried to look for a common pattern in search-related actions. Indeed, MMBag remains a search interface at its core. In order for users to interact efficiently with I~SEARCH, we needed a well-known interface paradigm. Accross the Web, a pattern has the monopoly for search related actions:  the text field, where a user can focus, enter her query, and trigger subsequent search actions. From big Web search engines such as Google, Yahoo or Bing, to small internal search engines, the pattern stays the same. 

However, I~SEARCH can't directly benefit from this broadly accepted pattern, as a multi-modal search engine must accept a large number of types of input at the same time: audio, video, 3D objects, sketches, etc. How can this be achieved? Some search engines, even if they don't have the need for true multi-modal querying, do have the need to accept input which is not plain text.

As a first example, we consider TinEye\footnote{\url{http://www.tineye.com/}}. TinEye is a Web-based search engine that allows for query by image content (QBIC) in order to retrieve similar or related images. The interface is split in two distinct parts: one part is a text box to provide a link to a Web-hosted image, while the second part allows for direct file upload (Figure~\ref{fig:tineye-ui}). This interface is a good solution for a search engine like TinEye (image input, image output) but I~SEARCH will need to come with more options than that.
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\linewidth]{resources/tineye-UI.png}
  \caption{Extract from the TinEye User Interface}
  \label{fig:tineye-ui}
\end{figure}

As a second example, we examine MMRetrieval\footnote{\url{http://www.mmretrieval.net}}~\cite{mmretrieval}. It is an attempt at bringing together image and text search to compose a multi-modal query. MMRetrieval is a good showcase for the problem of designing a user interface (UI) with many user-tweakable options. For a user from outside the Information Retrieval field, the UI seems not necessarily clear in all detail, especially when specific terms such as \emph{Image ARF} are used (Figure~\ref{fig:mmretrieval-ui}).

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\linewidth]{resources/mmretrieval-UI.png}
  \caption{Extract from the MMRetrieval User Interface}
  \label{fig:mmretrieval-ui}
\end{figure}

Finally, we have a look at Google Search-by-image, a functionality launched on June, 14, 2011\footnote{\url{http://googleblog.blogspot.com/2011/06/knocking-down-barriers-to-knowledge.html}} and has the same requirements as MMRetrieval in terms of user interface, i.e., combining text and image input. With the Search-by-image interface, Google has succeeded in keeping the text box pattern (Figure~\ref{fig:search-by-image-box}), while preventing any extra visual noise. The interface is \emph{progressively disclosed} to users via a contextual menu when the camera icon is clicked (Figure~\ref{fig:search-by-image-popup}).

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\linewidth]{resources/search-by-image-UI-box.png}
  \caption{Input for the Search-by-image UI}
  \label{fig:search-by-image-box}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\linewidth]{resources/search-by-image-UI-popup.png}
  \caption{Popup for the Search-by-image UI}
  \label{fig:search-by-image-popup}
\end{figure}

Even if the Search-by-image solution seems very elegant, this is still not suitable for I~SEARCH since the interface would require a high number of small icons: camera, 3D, geolocation, audio, video, etc.  As a result, we decided to adapt a solution that can be seen in Figure~\ref{fig:isearch-ui}. This interface keeps the idea of a single text box. It is enriched by auto-completion as well as ``tokenization". The term ``tokenization" refers to the process of representing an item (picture, sound, etc.) with a small token in the text field, as if it was part of the text query. We also kept the idea of progressive disclosure for the different actions required by the various modes, e.g., uploading a picture or sketching something. The different icons are grouped together in a separated menu, close to the main search field.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\linewidth]{resources/isearch-UI.png}
  \caption{First version of I~SEARCH interface}
  \label{fig:isearch-ui}
\end{figure}

\subsection{UIIFace}

(no matter what input device is used > one interface / one event for any device) 

\subsection{CoFind}

(enabling real-time collaborative search query creation on a pure html website)

\section{Implementation and Results}
(Jonas, Arnaud)

The I~SEARCH GUI is built using the Web platform. HTML, CSS and JavaScript are the three main building blocks for the interface. The rationale behind that choice is the following: I~SEARCH needs to be cross-browser and cross-device compatible. The latest specifications of those technologies enable us to do just that. Indeed, CSS3~\cite{css3}, HTML5~\cite{html5} and the therein defined new JavaScript APIs empower the browser in truly novel ways.

Our strategy also includes support for older browsers. Users sometimes don't have access to a cutting-edge browser. When a feature we use is not available for a certain browser, two choices are available: either drop support for that feature if it is not important (e.g., drop visual shims like CSS shadows or border-radius), or provide alternate solution to mimic the experience.

\emph{Side note: the plan is to go through each feature we are going to use to show that...maybe with code samples?}

\subsection{HTML5 Semantic Markup} 

HTML5 semantic markup, with no fallback for older browsers (older browsers won't understand the extra meaning, and that's totally fine)

\subsection{CSS3 Media Queries} 

CSS3 media-queries, with Javascript fallback for older browsers

\subsection{New Javascript APIs}

 Audio, Video and File APIs, with Flash fallback

\subsection{Canvas}

\subsection{Geolocation}

\subsection{Sensors}

\subsection{Device API}


\section{Evaluation}
(Paul)
Requirement: result criteria
Did you understand the interface
Did you understand the process
2 Use Cases: Rhythm clapping (UC1) and Games (UC7)?

To validate our interface design on tasks of multi-modal search we conducted a user study. 
We used a comparative study design to explore how the usage of different media types can 
influence the success rate of search queries. 

Overall, we expected that the user can handle the user interface very easily and gets a 
result which satisfies her needs. We therefore set the following hypothesis: 
\begin{enumerate}
  \item (H1) a search query will mostly contain just one modality
  \item (H2) a search refinement will be done by adding other media types
  \item (H3) a search refinement will not be done by removing query parts
  \item (H4) the use of multiple media items in a search query will result
  in faster search results compared to a single-media search
\end{enumerate}

For the user study we recruited N7N (FF,KK,TT,RR,2xGunar,Ohl) participants (NN male and NN female) aged between YYYY and YYYY. All participants were familiar with textual Web-based search. We asked all study participants to find NN different items (one audio file, one 3D model and one image, see Figure 1). All search queries are part of two uses cases which were described in section SSS. For explanation, what should be searched (and find), these items were shown in their original format to the study participants. To perform the search queries a mobile notebook with touch display was used. Consequently, for comparison with single-media search the study participants should find the same objects using common search single-media search engines. Our goal was to validate our interface design as well as to measure the impact of the possibility of multi-modal search. 

All users completed all tasks successfully. As a rough measure of task performance

\section{Conclusion and Future Work}
(Tom)

\section{Acknowledgments}
This work is partly funded by the EU FP7 I~SEARCH project under project reference 248296.

\bibliographystyle{plain}
\bibliography{mmm2012}
\end{document}