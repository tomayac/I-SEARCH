\documentclass{acm_proc_article-sp}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[activate=compatibility]{microtype}

% autoref command
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}

\usepackage{enumitem}

\usepackage{mathtools}

% give emph a normal fontsize
\let\oldemph\emph
\renewcommand{\emph}[1]{\oldemph{\fontsize{9}{9}\selectfont #1}}

% more readable footnote layout
\renewcommand{\footnotesize}{\fontsize{8pt}{10pt}}
\setlength{\footnotesep}{.5cm}

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{TODO}: #1{\bf \}}}}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=ยง,
        aboveskip=0.5em,belowskip=0em,abovecaptionskip=0em,belowcaptionskip=0em,
framexbottommargin=-1em,
        basicstyle=\ttfamily\listingsize\selectfont}

% use Courier from this point onward
\let\oldttdefault\ttdefault
\renewcommand{\ttdefault}{pcr}
\let\oldurl\url
\renewcommand{\url}[1]{\inlinelistingsize\oldurl{#1}}

% linewrap symbol
\definecolor{grey}{RGB}{130,130,130}
\newcommand{\linewrap}{\raisebox{-.6ex}{\textcolor{grey}{$\hookleftarrow$}}}

% more pleasing quote environment
\usepackage{tikz}
\newcommand*{\openquote}{\tikz[remember picture,overlay,xshift=-7pt,yshift=1pt]
     \node (OQ) {\fontfamily{fxl}\fontsize{16}{16}\selectfont``};\kern0pt}
\newcommand*{\closequote}{\tikz[remember picture,overlay,xshift=2pt,yshift=-4.5pt]
     \node (CQ) {\fontfamily{fxl}\fontsize{16}{16}\selectfont''};}
\renewenvironment{quote}%
{\setlength{\parindent}{1cm}\par\openquote}
{\closequote\vspace{-4.5pt}
}

\begin{document}

\title{I-SEARCH -- A Multimodal Search Engine based on\\*Rich Unified Content Description (RUCoD)}

\numberofauthors{11}
\author{
\alignauthor
Thomas Steiner\\
	\affaddr{Google Germany GmbH}\\
	\affaddr{ABC-Str. 19}\\
	\affaddr{20354 Hamburg, Germany}\\
	\email{tomac@google.com}
\alignauthor
Jonas Etzold\\
	\affaddr{FH Fulda}
\alignauthor
Paul Grimm\\	
	\affaddr{FH Fulda}
\and
Francesco Nucci\\
	\affaddr{ENG}
\alignauthor
Vincenzo Croce\\
	\affaddr{ENG}
\alignauthor
Antonio Camurri\\
	\affaddr{UNIGE}
}

\additionalauthors{Additional authors: 
  Thanassis Mademlis (ITI),
  Alberto Massari (UNIGE),
  Petros Daras (ITI),
  Apostolos Axenopoulos (ITI), and
  Dimitrios Tzovaras (ITI)
}
\maketitle

\begin{abstract}

\end{abstract}

\category{H.3.4}{Information Systems}{Information Storage and Retrieval}[World Wide Web]
\category{H.3.5}{Online Information Services}{Web-based services}

\keywords{Multimodality, Rich Unified Content Description, search engine}

\section{Introduction}
Since the beginning of the age of Web search engines in 1990, the search process is associated with a text input field.
From the first search engine, Archie~\cite{archie}, to state-of-the-art search engines like WolframAlpha~\cite{wolframalpha}, this fundamental input paradigm has not changed.
In a certain sense the search process has been revolutionized on mobile devices through the addition of voice input support like Apple's Siri~\cite{siri} for iOS, Google's Voice Actions~\cite{voiceactions} for Android, and through Voice Search~\cite{voicesearch} for desktop computers.
Support for the human voice as an input modality is mainly driven by shortcomings of (mobile) keyboards.
One modality, text, is simply replaced by another, voice.
However, what is still missing is a truly multimodal search engine.
If the searched-for item is slow, sad, minor scale piano music\footnote{Sad, slow piano music: \url{http://youtu.be/a_Am4cHMBKM}}, the best input modality might be to just upload a short sample (``audio'') and an unhappy smiley face (``emotion'').
When searching for the sound of Times Square, New York\footnote{Sound of Times Square: \url{http://youtu.be/_ttz9QDj0cw}}, the best input modality might be the coordinates (``geolocation'') of Times Square and a photo of a yellow cab (``image'').
Finally, when searching for a Greek-looking font\footnote{Greek-looking font: \url{http://www.dafont.com/cleopatra.font}}, the best input modality might be to just scribble some letters in the desired style (``sketch'').
The outlined search scenarios are of very different nature, and even for human beings it is not easy to find \emph{the} correct answer, let alone that such answer exists for each scenario.
With \mbox{I-SEARCH}, we thus strive for a paradigm shift; away from textual term-based search towards a more explorative multimodality-driven search experience.

It is evident that for the outlined scenarios to work, a significant investment in describing the underlying media items is necessary.
Therefore, in~\cite{ijmis2010}, we have first introduced the concept of so-called \emph{content objects}, and second, a description format named \emph{Rich Unified Content Description (RUCoD)}.
Content objects are rich media presentations, enclosing different types of media, along with real-world information and user-related information.
RUCoD provides a uniform descriptor for all types of content objects, irrespective of the underlying media and accompanying information.

In this paper, we give an overview on the \mbox{I-SEARCH} project so far.
In \autoref{sec:projectgoals}, we outline the general objectives of  \mbox{I-SEARCH}.
\autoref{sec:projectresults} highlights significant achievements.
We describe the details of our system in \autoref{sec:systemdemonstration}.
Relevant related work is shown in \autoref{sec:relatedwork}.
We give an outlook on future work in \autoref{sec:futurework}.
The paper is closed with a conclusion in \autoref{sec:conclusion}.

\section{Project Goals} \label{sec:projectgoals}


\section{Project Results} \label{sec:projectresults}
\url{http://www.isearch-project.eu/isearch/RUCoD/RUCoD_Descriptors.xsd}
\url{http://www.isearch-project.eu/isearch/RUCoD/RUCoD.xsd}

\section{System Demonstration} \label{sec:systemdemonstration}


\section{Related Work} \label{sec:relatedwork}
We start covering related work with a differentiation of terms.
\emph{Multimodal search} can be used in two senses; (i), in the sense of multimodal result output based on unimodal query input, and (ii), in the sense of both multimodal result output and multimodal query input.
We follow the second definition, i.e., require the query input interface to allow for multimodality.
We do not consider the first definition, as common search engines today already by default return multimodal results.

An interesting multimodal search engine was developed in the scope of the PHAROS European project~\cite{pharos2008}. While the initial query is still keyword-based, the search engine allows for refinements in form of facets, like location, that can be considered modalities.
\mbox{I-SEARCH} develops this concept one step further by supporting multimodality to start with.

In~\cite{multimodalitysun}, Rahn Frederick discusses the importance of multimodality in search-driven on-device portals, i.e., handset-resident mobile applications, often preloaded, that enhance the discovery and consumption of endorsed mobile content, services, and applications.
Consumers can navigate on-device portals by searching with text, voice, and camera images.
Rahn Frederick's article is relevant to \mbox{I-SEARCH}, as it is specifically focused on mobile devices, albeit the scope of our project is further in the sense of also covering desktop devices. 

In a W3C Note~\cite{w3cmultimodal2003} , Larson \textit{et al.} describe a multimodal interaction framework, and identify the major components for multimodal systems.
The multimodal interaction framework is not an architecture \textit{per se}, but rather a level of abstraction above an architecture and identifies the markup languages used to describe information required by components and for data flows among components.

With Mudra~\cite{mudra2011}, Hoste \textit{et al.} present a unified multimodal interaction framework supporting the integrated processing of low-level data streams as well as high-level semantic inferences.
Their architecture is designed to support a growing set of input modalities as well as to enable the integration of existing or novel multimodal fusion engines.
Input fusion engines combine and interpret data from multiple input modalities in a parallel or sequential way.
With \mbox{I-SEARCH}, we present a search engine that captures modalities sequentially, however, processes them in parallel.

\section{Future Work} \label{sec:futurework}


\section{Conclusion} \label{sec:conclusion}


\section{Acknowledgments}
This work was partially supported by the European Commission under Grant No. 248296 FP7 \mbox{I-SEARCH} project.

% back to normal size Computer Modern for URLs in bibliography
\let\ttdefault\oldttdefault
\let\url\oldurl

\bibliographystyle{abbrv}
\bibliography{www2012}

\balancecolumns
\end{document}